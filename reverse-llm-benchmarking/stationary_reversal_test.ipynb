{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stationary_reversal' from '/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import stationary_reversal as sr\n",
    "reload(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "          \"EleutherAI/pythia-160m-deduped-v0\"\n",
    "      ).to(device)\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "prefix_length  = 1\n",
    "suffix = \" Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "# tokenized_suffix = tokenized_suffix.unsqueeze(0)\n",
    "suffix_length = len(tokenized_suffix[0])\n",
    "empirical_dist = torch.load(\"../data/pile10k_empirical.pt\")\n",
    "vocab_size = empirical_dist.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:56<00:00,  5.50s/it]\n",
      "/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal.py:114: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = torch.distributions.Categorical(torch.nn.functional.softmax(vector_of_logprobs[prefix_length - i - 1,:])).sample()\n"
     ]
    }
   ],
   "source": [
    "Obama_log_probs_batch = sr.stationary_reverse_full_dist(\n",
    "  model, empirical_dist, prefix_length, tokenized_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Barack of the, and President in. that to']\n"
     ]
    }
   ],
   "source": [
    "# top k tokens for Obama_log_probs_batch\n",
    "k = 10\n",
    "top_k_tokens = torch.topk(Obama_log_probs_batch, k, dim=1).indices\n",
    "top_k_tokens = top_k_tokens.cpu().numpy()\n",
    "top_k_tokens = tokenizer.batch_decode(top_k_tokens, skip_special_tokens=True)\n",
    "print(top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"afterless/reverse-pythia-160m\"\n",
    ").to(device)\n",
    "reverse_model.eval()\n",
    "rev_out = reverse_model(input_ids=tokenized_suffix)\n",
    "rev_logits = rev_out.logits\n",
    "rev_logprobs = torch.nn.functional.log_softmax(rev_logits, dim=-1)\n",
    "rev_probs = torch.exp(rev_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k tokens for rev_probs\n",
    "k = 10\n",
    "rev_topk = torch.topk(rev_probs, k, dim=-1)\n",
    "rev_topk_tokens = rev_topk.indices\n",
    "rev_topk_probs = rev_topk.values\n",
    "rev_topk_words = tokenizer.decode(rev_topk_tokens[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Barack of the and for that, an from by'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_topk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obama_probs = torch.exp(Obama_log_probs_batch)\n",
    "stat_total_variation = 0.5*torch.sum(torch.abs(Obama_probs[0,:] - rev_probs[0,0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dist = (1.0/vocab_size)+torch.zeros(vocab_size)\n",
    "unif_total_variation = 0.5*torch.sum(torch.abs(uniform_dist - rev_probs[0,0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024320\n",
      "i= 0\n",
      "3823104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132048\n",
      "3321271296\n",
      "3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/64 [00:06<06:27,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/64 [00:11<11:52, 11.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m reload(sr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenized_suffix_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((torch\u001b[39m.\u001b[39mzeros([\u001b[39m1\u001b[39m,\u001b[39m20\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32) ,tokenized_suffix),dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_suffix \u001b[39m=\u001b[39m sr\u001b[39m.\u001b[39;49mstationary_reverse_full_dist_suffix_calculation(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m       model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m       empirical_dist, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m       tokenized_suffix_test,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal_test.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m       vocab_batch_size\u001b[39m=\u001b[39;49m\u001b[39m786\u001b[39;49m)\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal.py:140\u001b[0m, in \u001b[0;36mstationary_reverse_full_dist_suffix_calculation\u001b[0;34m(model, stationary_dist, tokenized_suffix, vocab_batch_size, renormalize_dist)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mnumel(vector_of_logprobs) \u001b[39m*\u001b[39m vector_of_logprobs\u001b[39m.\u001b[39melement_size())\n\u001b[1;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(suffix_length\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 140\u001b[0m     vector_of_logprobs[i,:] \u001b[39m=\u001b[39m stationary_reverse_full_dist(\n\u001b[1;32m    141\u001b[0m         model,\n\u001b[1;32m    142\u001b[0m         stationary_dist, \n\u001b[1;32m    143\u001b[0m         \u001b[39m1\u001b[39;49m, \n\u001b[1;32m    144\u001b[0m         tokenized_suffix[:,i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m:],\n\u001b[1;32m    145\u001b[0m         vocab_batch_size\u001b[39m=\u001b[39;49mvocab_batch_size,\n\u001b[1;32m    146\u001b[0m         renormalize_dist\u001b[39m=\u001b[39;49mrenormalize_dist)[\u001b[39m0\u001b[39m,:]\n\u001b[1;32m    147\u001b[0m     gc\u001b[39m.\u001b[39mcollect()       \n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m vector_of_logprobs\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal.py:94\u001b[0m, in \u001b[0;36mstationary_reverse_full_dist\u001b[0;34m(model, stationary_dist, prefix_length, tokenized_suffix, vocab_batch_size, renormalize_dist)\u001b[0m\n\u001b[1;32m     92\u001b[0m v_sentences \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((batch_indices\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), splus\u001b[39m.\u001b[39mrepeat(batch_indices\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mnumel(v_sentences) \u001b[39m*\u001b[39m v_sentences\u001b[39m.\u001b[39melement_size())\n\u001b[0;32m---> 94\u001b[0m psp_given_v_batch \u001b[39m=\u001b[39m get_cond_logprob(v_sentences, model)\n\u001b[1;32m     95\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mnumel(psp_given_v_batch) \u001b[39m*\u001b[39m psp_given_v_batch\u001b[39m.\u001b[39melement_size())\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Check if it's the last batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/stationary_reversal.py:18\u001b[0m, in \u001b[0;36mget_cond_logprob\u001b[0;34m(input_ids, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cond_logprob\u001b[39m(input_ids, model):\n\u001b[1;32m     17\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 18\u001b[0m         logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(model(input_ids\u001b[39m=\u001b[39;49minput_ids)\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m         \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mnumel(logprobs) \u001b[39m*\u001b[39m logprobs\u001b[39m.\u001b[39melement_size())\n\u001b[1;32m     21\u001b[0m     \u001b[39m# Get the log probabilities corresponding to the words in input_ids\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:777\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    763\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgpt_neox(\n\u001b[1;32m    764\u001b[0m     input_ids,\n\u001b[1;32m    765\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    773\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    774\u001b[0m )\n\u001b[1;32m    776\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 777\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_out(hidden_states)\n\u001b[1;32m    779\u001b[0m lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[39m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import stationary_reversal as sr\n",
    "reload(sr)\n",
    "\n",
    "tokenized_suffix_test = torch.cat((torch.zeros([1,20], dtype=torch.int32) ,tokenized_suffix),dim=-1)\n",
    "test_suffix = sr.stationary_reverse_full_dist_suffix_calculation(\n",
    "      model,\n",
    "      empirical_dist, \n",
    "      tokenized_suffix_test,\n",
    "      vocab_batch_size=786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_suffix_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5*(torch.sum(torch.abs(torch.exp(test_suffix[1,:])-Obama_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50304])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_suffix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversing-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
