{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stationary_reversal' from 'c:\\\\Users\\\\abhay\\\\Documents\\\\research\\\\reverse-dynamics-nlp\\\\reverse-llm-benchmarking\\\\stationary_reversal.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import stationary_reversal as sr\n",
    "reload(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m-deduped\",\n",
    "    revision=\"step3000\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_length = 20\n",
    "suffix = \" Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "# tokenized_suffix = tokenized_suffix.unsqueeze(0)\n",
    "suffix_length = len(tokenized_suffix[0])\n",
    "empirical_dist = torch.load(\"../data/pi-pile10k-pythia160m.pt\").cuda()\n",
    "empirical_dist = torch.ones_like(empirical_dist) / empirical_dist.shape[0]\n",
    "vocab_size = empirical_dist.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [00:24<00:00, 15.72it/s]\n",
      "100%|██████████| 393/393 [00:25<00:00, 15.49it/s]\n",
      "100%|██████████| 393/393 [00:28<00:00, 13.93it/s]\n",
      "100%|██████████| 393/393 [00:28<00:00, 13.61it/s]\n",
      "100%|██████████| 393/393 [00:35<00:00, 10.92it/s]\n",
      "100%|██████████| 393/393 [00:39<00:00,  9.90it/s]\n",
      "100%|██████████| 393/393 [00:42<00:00,  9.34it/s]\n",
      "100%|██████████| 393/393 [00:47<00:00,  8.20it/s]\n",
      "100%|██████████| 393/393 [00:38<00:00, 10.16it/s]\n",
      "100%|██████████| 393/393 [00:55<00:00,  7.11it/s]\n",
      "100%|██████████| 393/393 [01:00<00:00,  6.51it/s]\n",
      "100%|██████████| 393/393 [00:57<00:00,  6.83it/s]\n",
      "100%|██████████| 393/393 [01:05<00:00,  5.97it/s]\n",
      "100%|██████████| 393/393 [01:11<00:00,  5.52it/s]\n",
      "100%|██████████| 393/393 [01:12<00:00,  5.45it/s]\n",
      "100%|██████████| 393/393 [01:17<00:00,  5.06it/s]\n",
      "100%|██████████| 393/393 [01:24<00:00,  4.67it/s]\n",
      "100%|██████████| 393/393 [01:27<00:00,  4.50it/s]\n",
      "100%|██████████| 393/393 [01:29<00:00,  4.39it/s]\n",
      "100%|██████████| 393/393 [01:36<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from reverse_sampling import sample_reverse_dynamics\n",
    "\n",
    "output = sample_reverse_dynamics(\n",
    "    model,\n",
    "    empirical_dist,\n",
    "    prefix_length,\n",
    "    tokenized_suffix,\n",
    "    temperature=0.7,\n",
    "    vocab_batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APIAPIAPIEnlambdaongongang Africa,” with the women of Sudan’s savannah, Obama'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "vocab_size = empirical_dist.shape[0]\n",
    "posterior = torch.zeros(vocab_size)\n",
    "total_batches = math.ceil(vocab_size / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from stationary_reversal import get_logprob\n",
    "\n",
    "outs = []\n",
    "\n",
    "for batch_num in tqdm(range(total_batches)):\n",
    "    start_idx = batch_num * 1024\n",
    "    end_idx = start_idx + 1024\n",
    "\n",
    "    batch_indices = (\n",
    "        torch.arange(start_idx, min(end_idx, vocab_size) ).clamp(0, vocab_size - 1).to(device)\n",
    "    )\n",
    "    v_sentences = torch.cat(\n",
    "        (batch_indices.unsqueeze(1), tokenized_suffix.repeat(batch_indices.size(0), 1)),\n",
    "        dim=-1,\n",
    "    )\n",
    "    \n",
    "    probs = get_logprob(v_sentences, model, empirical_dist.cuda())\n",
    "    outs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22306, device='cuda:0')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_with_temp(distribution, temperature):\n",
    "    if temperature == 0:\n",
    "        p = distribution.argmax()\n",
    "    else:\n",
    "        p = torch.distributions.Categorical(\n",
    "            logits = distribution / temperature\n",
    "        ).sample()\n",
    "    return p\n",
    "\n",
    "sample_with_temp(torch.cat(outs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mem'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample_with_temp(torch.cat(outs), 1.0).unsqueeze(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-25.7309, -20.9324, -20.8300, -25.0748, -22.0734, -21.5063, -21.6011,\n",
       "        -24.1762, -22.5759, -25.6313, -22.3087, -23.7663, -23.6850, -23.7977,\n",
       "        -22.4933, -23.2035, -25.0301, -23.6488, -21.2583, -25.0870, -21.4564,\n",
       "        -20.7695, -22.6948, -22.2432, -20.6374, -23.7555, -23.2779, -26.2181,\n",
       "        -23.1861, -21.4037, -24.6500, -19.4641, -22.4541, -23.3352, -22.4891,\n",
       "        -23.9108, -22.7646, -25.2841, -22.6171, -24.0714, -23.7114, -22.8568,\n",
       "        -21.5846, -25.0019, -22.4097, -23.0850, -22.8781, -21.9645, -23.3015,\n",
       "        -21.9750, -25.3870, -21.3300, -24.3144, -22.2731, -22.1573, -21.3933,\n",
       "        -23.4267, -23.4722, -22.9275, -20.3732, -20.9247, -24.3184, -20.8503,\n",
       "        -20.8255, -22.0150, -22.6773, -21.8333, -24.4127, -26.7027, -21.5877,\n",
       "        -22.2574, -24.1322, -23.0682, -25.8654, -24.6825, -24.6051, -22.7932,\n",
       "        -23.5175, -27.0671, -24.1986, -22.8315, -23.1738, -25.2061, -23.4893,\n",
       "        -23.7136, -23.0180, -25.2781, -25.2111, -24.0354, -24.0675, -24.9220,\n",
       "        -24.9104, -24.3331, -23.6363, -26.1302, -23.9145, -25.1732, -24.8057,\n",
       "        -25.9665, -24.8239, -26.6242, -23.4666, -21.1096, -20.1666, -22.5337,\n",
       "        -23.4510, -21.5062, -22.1701, -21.7432, -23.1897, -23.3676, -22.1742,\n",
       "        -21.1316, -22.2185, -24.2263, -21.4152, -22.8329, -22.6155, -22.1893,\n",
       "        -20.6178, -23.2463, -19.3613, -22.4747, -20.8121, -22.1222, -23.2857,\n",
       "        -22.3258, -21.9242], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.125"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size /1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversing-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
