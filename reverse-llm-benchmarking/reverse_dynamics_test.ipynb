{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'stationary_reversal' from 'c:\\\\Users\\\\abhay\\\\Documents\\\\research\\\\reverse-dynamics-nlp\\\\reverse-llm-benchmarking\\\\stationary_reversal.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import stationary_reversal as sr\n",
    "reload(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m-deduped\",\n",
    "    revision=\"step3000\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_length = 10\n",
    "suffix = \" Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "# tokenized_suffix = tokenized_suffix.unsqueeze(0)\n",
    "suffix_length = len(tokenized_suffix[0])\n",
    "empirical_dist = torch.load(\"..\\data\\pythia-160m-deduped-v0_stationary_dist.pt\").cuda()\n",
    "\n",
    "vocab_size = empirical_dist.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dist = torch.ones_like(empirical_dist) / empirical_dist.shape[0]\n",
    "empirical_dist = empirical_dist * 0.8 + uniform_dist * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 786/786 [00:31<00:00, 24.63it/s]\n",
      "100%|██████████| 786/786 [00:34<00:00, 22.72it/s]\n",
      "100%|██████████| 786/786 [00:22<00:00, 34.81it/s]\n",
      "100%|██████████| 786/786 [00:20<00:00, 38.51it/s]\n",
      "100%|██████████| 786/786 [00:24<00:00, 31.46it/s]\n",
      "100%|██████████| 786/786 [00:36<00:00, 21.25it/s]\n",
      "100%|██████████| 786/786 [01:00<00:00, 13.05it/s]\n",
      "100%|██████████| 786/786 [01:03<00:00, 12.37it/s]\n",
      "100%|██████████| 786/786 [01:14<00:00, 10.55it/s]\n",
      "100%|██████████| 786/786 [00:38<00:00, 20.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from reverse_sampling import sample_reverse_dynamics\n",
    "\n",
    "output = sample_reverse_dynamics(\n",
    "    model,\n",
    "    empirical_dist,\n",
    "    prefix_length,\n",
    "    tokenized_suffix,\n",
    "    temperature=0.7,\n",
    "    vocab_batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" {#sec2. Mueller's separate interviews of president Obama\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "vocab_size = empirical_dist.shape[0]\n",
    "posterior = torch.zeros(vocab_size)\n",
    "total_batches = math.ceil(vocab_size / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from stationary_reversal import get_logprob\n",
    "\n",
    "outs = []\n",
    "\n",
    "for batch_num in tqdm(range(total_batches)):\n",
    "    start_idx = batch_num * 1024\n",
    "    end_idx = start_idx + 1024\n",
    "\n",
    "    batch_indices = (\n",
    "        torch.arange(start_idx, min(end_idx, vocab_size) ).clamp(0, vocab_size - 1).to(device)\n",
    "    )\n",
    "    v_sentences = torch.cat(\n",
    "        (batch_indices.unsqueeze(1), tokenized_suffix.repeat(batch_indices.size(0), 1)),\n",
    "        dim=-1,\n",
    "    )\n",
    "    \n",
    "    probs = get_logprob(v_sentences, model, empirical_dist.cuda())\n",
    "    outs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_with_temp(distribution, temperature):\n",
    "    if temperature == 0:\n",
    "        p = distribution.argmax()\n",
    "    else:\n",
    "        p = torch.distributions.Categorical(\n",
    "            logits = distribution / temperature\n",
    "        ).sample()\n",
    "    return p\n",
    "\n",
    "sample_with_temp(torch.cat(outs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample_with_temp(torch.cat(outs), 0.5).unsqueeze(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-27.0615, -22.3925, -22.1784, -26.3934, -23.5982, -22.7611, -22.8974,\n",
       "        -25.6003, -23.9575, -27.0090, -23.6367, -25.1839, -25.0625, -25.0923,\n",
       "        -23.8681, -24.6099, -26.5199, -25.0262, -22.6178, -26.5734, -22.8094,\n",
       "        -22.2456, -23.9815, -23.6188, -22.0856, -25.2053, -24.6022, -27.6299,\n",
       "        -24.5784, -22.7017, -25.9854, -20.6962, -23.6919, -24.7146, -23.8885,\n",
       "        -25.4163, -24.1862, -26.6214, -24.0831, -25.4412, -25.1563, -24.3024,\n",
       "        -23.0253, -26.4156, -23.8638, -24.3872, -24.3518, -23.1973, -24.6612,\n",
       "        -23.4344, -26.8461, -22.7198, -25.4789, -23.5496, -23.6457, -22.7627,\n",
       "        -24.8559, -24.9498, -24.2420, -21.7104, -22.1432, -25.6042, -22.1379,\n",
       "        -22.2354, -23.2504, -24.0490, -23.1965, -25.6685, -28.0224, -22.8650,\n",
       "        -23.7653, -25.4745, -24.4520, -27.1851, -26.0717, -25.7205, -24.1750,\n",
       "        -24.8818, -23.1033, -24.2869, -22.9034, -22.9289, -24.0184, -23.0561,\n",
       "        -22.8144, -22.4728, -23.2446, -24.6217, -23.2994, -23.3036, -22.4234,\n",
       "        -24.1657, -22.9614, -22.1172, -22.5630, -21.9238, -22.5099, -22.5916,\n",
       "        -21.9770, -21.7642, -22.6334, -25.0761, -22.7190, -21.7760, -24.1431,\n",
       "        -25.0604, -23.1156, -23.7795, -23.3526, -24.7991, -24.9771, -23.7836,\n",
       "        -22.7410, -23.8279, -25.8357, -23.0246, -24.4423, -24.2250, -23.7987,\n",
       "        -22.2272, -24.8557, -20.9707, -24.0841, -22.4215, -23.7316, -24.8951,\n",
       "        -23.9352, -23.5195], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size /1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversing-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
