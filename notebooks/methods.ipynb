{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from src import *\n",
    "\n",
    "model_size = \"160m\"\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-160m-deduped/step3000\",\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "\n",
    "reverse_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\"\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      " should never be president\n"
     ]
    }
   ],
   "source": [
    "input_str = \" !\" * 15\n",
    "expected_output = \" should never be president\"\n",
    "\n",
    "# Reinitialize string to be a sample from RLM\n",
    "#rlm = ReverseModelSampler(model, reverse_model, tokenizer, num_beams=1)\n",
    "#input_str = rlm.optimize(input_str, expected_output, temperature=0.01).split(expected_output)[0]\n",
    "\n",
    "print(input_str.replace(\"\\n\", \"\"))\n",
    "print(expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCG based methods\n",
    "gcg = GreedyCoordinateGradient(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    n_top_indices=256,\n",
    "    n_proposals=512,\n",
    "    n_epochs=512,\n",
    "    prefix_loss_weight=-1,\n",
    "    revert_on_loss_increase=False\n",
    ")\n",
    "gcg_reg = GreedyCoordinateGradient(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    n_top_indices=128,\n",
    "    n_proposals=512,\n",
    "    n_epochs=512,\n",
    "    prefix_loss_weight=1,\n",
    "    revert_on_loss_increase=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCG:  should ReportsNever=\"../../../../../../392 seventeen ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------!}}}PRESubottu Usually presidents=\"{{They should never be president\n",
      "GCG Reg:  nÃ А USDAР Temperature arguably intendimg Before fMRI Creates Irish People Why Biden should never be president\n"
     ]
    }
   ],
   "source": [
    "output1 = gcg.optimize(input_str, expected_output)\n",
    "print(\"GCG:\", output1.replace(\"\\n\", \"\"))\n",
    "output2 = gcg_reg.optimize(input_str, expected_output)\n",
    "print(\"GCG Reg:\", output2.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' should ReportsNever=\"../../../../../../392 seventeen ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------!}}}PRESubottu Usually presidents=\"{{They should have been members of the community. Been in the past***/\\n                                                                       OF THE SUPRE'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(\n",
    "        input_ids=tokenizer.encode(\n",
    "            output1.replace(expected_output, \"\"),\n",
    "            return_tensors=\"pt\").cuda(),\n",
    "            max_new_tokens=25\n",
    "    )[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Reversal: i -------------------------- ------------------------------ ------------------------------ ------------------------------ ----------- ----------- ----------- Inter Interribe at anytime, he should never be president\n"
     ]
    }
   ],
   "source": [
    "dist = torch.load(\"data/distributions/pile_empirical.pt\")\n",
    "bayes = ReversalEmpiricalPrior(model, dist, tokenizer, reverse_model=reverse_model, num_top_tokens=10_000)\n",
    "output3 = bayes.optimize(input_str, expected_output, temperature=0.7)\n",
    "print(\"Bayesian Reversal:\", output3.replace(\"\\n\", \"\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output3tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput3tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      3\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m      4\u001b[0m             output3\u001b[38;5;241m.\u001b[39mreplace(expected_output, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m             return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      6\u001b[0m     )[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output3tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "output3tokenizer.decode(\n",
    "    model.generate(\n",
    "        input_ids=tokenizer.encode(\n",
    "            output3.replace(expected_output, \"\"),\n",
    "            return_tensors=\"pt\").cuda(),\n",
    "    )[0],\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLM Sampler:  period in history, yet there are still many Americans who believe that President Obama should never be president\n"
     ]
    }
   ],
   "source": [
    "rlm = ReverseModelSampler(model, reverse_model, tokenizer, num_beams=10)\n",
    "output4 = rlm.optimize(input_str, expected_output, temperature=1)\n",
    "print(\"RLM Sampler:\", output4.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix:  should never be president\n",
      "GCG Output:  Û!!!!!!!!! Justice welfare!!� should never be president\n",
      "GCG Reg Output:   duty!!!!!!!!!!!!!! should never be president\n",
      "Bayesian Reversal Output: , I think there was something else in placeholder.He's right Donald Trump should never be president\n",
      "Reverse LM Output:  period in history, yet there are still many Americans who believe that President Obama should never be president\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Suffix:\", expected_output)\n",
    "print(\"GCG Output: \", output1.replace(\"\\n\", \"\"))\n",
    "print(\"GCG Reg Output: \", output2.replace(\"\\n\", \"\"))\n",
    "print(\"Bayesian Reversal Output:\", output3.replace(\"\\n\", \"\"))\n",
    "print(\"Reverse LM Output:\", output4.replace(\"\\n\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
