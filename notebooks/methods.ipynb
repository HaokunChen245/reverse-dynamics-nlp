{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from src import *\n",
    "\n",
    "model_size = \"160m\"\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    ").cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4b-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-160m-deduped/step3000\",\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "\n",
    "reverse_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\"\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \" !\" * 15\n",
    "expected_output = \" should never be president\"\n",
    "\n",
    "def generate_from_reversal(output):\n",
    "    return tokenizer.decode(\n",
    "        model.generate(\n",
    "            input_ids=tokenizer.encode(\n",
    "                output.replace(expected_output, \"\"),\n",
    "                return_tensors=\"pt\").cuda(),\n",
    "                max_new_tokens=25\n",
    "        )[0],\n",
    "    ).replace(output.replace(expected_output, \"\"), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCG based methods\n",
    "gcg = GreedyCoordinateGradient(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    n_top_indices=256,\n",
    "    n_proposals=512,\n",
    "    n_epochs=512,\n",
    "    prefix_loss_weight=-1,\n",
    "    revert_on_loss_increase=False\n",
    ")\n",
    "gcg_reg = GreedyCoordinateGradient(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    n_top_indices=128,\n",
    "    n_proposals=512,\n",
    "    n_epochs=512,\n",
    "    prefix_loss_weight=1,\n",
    "    revert_on_loss_increase=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCG: (% weird emotion}}}}\\ Clinton [**rible!decisionSTARHow unacceptable Tex! You should never be president\n",
      "GCG Reg:  Face Scholar LoceiptIAL Figures ZipCompile $({\\ hated Trump deeply and he / should never be president\n"
     ]
    }
   ],
   "source": [
    "output1 = gcg.optimize(input_str, expected_output)\n",
    "print(\"GCG:\", output1.replace(\"\\n\", \"\"))\n",
    "output2 = gcg_reg.optimize(input_str, expected_output)\n",
    "print(\"GCG Reg:\", output2.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Reversal:  again, that Mueller’s should be contempt of Congress, and that he should never be president\n"
     ]
    }
   ],
   "source": [
    "dist = torch.load(\"data/distributions/pile_empirical.pt\")\n",
    "bayes = ReversalEmpiricalPrior(\n",
    "    model,\n",
    "    dist,\n",
    "    tokenizer,\n",
    "    reverse_model=reverse_model,\n",
    "    num_top_tokens=10_000,\n",
    "    batch_size=1024,\n",
    ")\n",
    "output3 = bayes.optimize(input_str, expected_output, temperature=0.5)\n",
    "print(\"Bayesian Reversal:\", output3.replace(\"\\n\", \"\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLM Sampler: However, according to Rabbi Joseph Smith, who is of the view that Trump should never be president\n"
     ]
    }
   ],
   "source": [
    "rlm = ReverseModelSampler(\n",
    "    model,\n",
    "    reverse_model,\n",
    "    tokenizer,\n",
    "    num_beams=10\n",
    ")\n",
    "output4 = rlm.optimize(input_str, expected_output, temperature=1)\n",
    "print(\"RLM Sampler:\", output4.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix:  should never be president\n",
      "GCG Output:  (% weird emotion}}}}\\ Clinton [**rible!decisionSTARHow unacceptable Tex! You should never be president\n",
      "GCG Reg Output:   Face Scholar LoceiptIAL Figures ZipCompile $({\\ hated Trump deeply and he / should never be president\n",
      "Bayesian Reversal Output:  again, that Mueller’s should be contempt of Congress, and that he should never be president\n",
      "Reverse LM Output: However, according to Rabbi Joseph Smith, who is of the view that Trump should never be president\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Suffix:\", expected_output)\n",
    "print(\"GCG Output: \", output1.replace(\"\\n\", \"\"))\n",
    "print(\"GCG Reg Output: \", output2.replace(\"\\n\", \"\"))\n",
    "print(\"Bayesian Reversal Output:\", output3.replace(\"\\n\", \"\"))\n",
    "print(\"Reverse LM Output:\", output4.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCG Output: -1.849\n",
      "GCG Reg Output: -4.253\n",
      "Bayesian Reversal Output: -11.191\n",
      "Reverse LM Output: -10.114\n"
     ]
    }
   ],
   "source": [
    "_, loss1 = forward_loss(model, (output1.replace(expected_output, \"\"), expected_output), tokenizer, loss=torch.nn.CrossEntropyLoss(reduction=\"sum\"),)\n",
    "_, loss2 = forward_loss(model, (output2.replace(expected_output, \"\"), expected_output), tokenizer, loss=torch.nn.CrossEntropyLoss(reduction=\"sum\"),)\n",
    "_, loss3 = forward_loss(model, (output3.replace(expected_output, \"\"), expected_output), tokenizer, loss=torch.nn.CrossEntropyLoss(reduction=\"sum\"),)\n",
    "_, loss4 = forward_loss(model, (output4.replace(expected_output, \"\"), expected_output), tokenizer, loss=torch.nn.CrossEntropyLoss(reduction=\"sum\"),)\n",
    "\n",
    "print(f\"GCG Output: {-loss1.item():.3f}\")\n",
    "print(f\"GCG Reg Output: {-loss2.item():.3f}\")\n",
    "print(f\"Bayesian Reversal Output: {-loss3.item():.3f}\")\n",
    "print(f\"Reverse LM Output: {-loss4.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix:  should never be president\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCG Output:   should never be president!**]{} Trump [**I’m not a big fan of her either!**]{}The second is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCG Reg Output:   should be impeached and removed from office, but he has not yet done so. He has not yet done so. He\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Reversal Output: , above all, a woman.The first time I met her, I was in the middle of a long,\n",
      "Reverse LM Output:  is a false prophet, the Lord has not revealed to him that Trump is a false prophet.The Lord has not\n"
     ]
    }
   ],
   "source": [
    "print(\"Suffix:\", expected_output)\n",
    "print(\"GCG Output: \", generate_from_reversal(output1).replace(\"\\n\", \"\"))\n",
    "print(\"GCG Reg Output: \", generate_from_reversal(output2).replace(\"\\n\", \"\"))\n",
    "print(\"Bayesian Reversal Output:\", generate_from_reversal(output3).replace(\"\\n\", \"\"))\n",
    "print(\"Reverse LM Output:\", generate_from_reversal(output4).replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
