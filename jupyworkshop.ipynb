{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_list = os.chdir('./../reverse-dynamics-nlp/')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM\n",
    "from prompt_optimizer import PromptOptimizer\n",
    "from utils import reverse_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Any\n",
    "def get_reverse_pair(dataset: Iterable[Any], chunk_func: Callable[..., Any], tokenizer: AutoTokenizer):\n",
    "    for chunk in dataset:\n",
    "        for chunk in chunk_func(chunk, tokenizer):\n",
    "            yield chunk\n",
    "\n",
    "def end_chunk_hf(chunk, tokenizer):\n",
    "    chunk = chunk['text']\n",
    "    tokens = tokenizer(chunk[-200:])['input_ids'][2:] #drop first couple tokens given risk of incomplete token\n",
    "    yield tokenizer.decode(tokens[-40:-30]), tokenizer.decode(tokens[-30:])\n",
    "\n",
    "def start_chunk_hf(chunk, tokenizer, num_prefix_tokens=10, num_suffix_tokens=40):\n",
    "    chunk = chunk['text']\n",
    "    tokens = tokenizer(chunk[:200])['input_ids'] #drop first couple tokens given risk of incomplete token\n",
    "    yield tokenizer.decode(tokens[:num_prefix_tokens]), tokenizer.decode(tokens[num_prefix_tokens:num_prefix_tokens+num_suffix_tokens])\n",
    "\n",
    "def rand_init(seq_length: int, tokenizer):\n",
    "    return tokenizer.decode(torch.randint(0, tokenizer.vocab_size, (seq_length,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jp6263/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 469.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"NeelNanda/pile-10k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"afterless/reverse-pythia-160m\")\n",
    "pairs = get_reverse_pair(dataset['train'], start_chunk_hf, tokenizer)\n",
    "print(next(pairs))\n",
    "bwd_model = GPTNeoXForCausalLM.from_pretrained(\"afterless/reverse-pythia-160m\").cuda()\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\", cache_dir='/scratch/jp6263/hf/models/').cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(model, pair, loss=torch.nn.CrossEntropyLoss(),):\n",
    "    prefix, suffix = pair\n",
    "    whole_tensor = tokenizer(prefix+suffix, return_tensors='pt').input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        logs = model(whole_tensor).logits\n",
    "    start_ind = len(tokenizer.encode(prefix))\n",
    "    l_pref = loss(logs[0,:start_ind], whole_tensor[0,1:start_ind+1])\n",
    "    l_suff = loss(logs[0,start_ind:-1], whole_tensor[0,start_ind+1:])\n",
    "    return l_pref, l_suff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcg = PromptOptimizer(model, tokenizer, n_proposals=128, n_epochs=500, n_top_indices=256, prefix_loss_weight=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "package tk.woppo.sunday. \n",
      "\n",
      "Predicted prefix:\n",
      "PlayingDat\n",
      "for suffix:\n",
      " model;\n",
      "\n",
      "import android.database.Cursor;\n",
      "\n",
      "import com.google.gson.Gson;\n",
      "import com.google.gson.annotations.SerializedName;\n",
      "\n",
      "Loss for suffix given predicted prefix is 0.7689047455787659 \n",
      " Suffix loss for true prefix is 0.6816657185554504\n",
      "NLL on predicted prefix is 10.96220588684082 \n",
      " NLL on true prefix is 5.973823547363281\n",
      "True prefix is:\n",
      "/*\n",
      " * Copyright 2010-2013 Amazon.com \n",
      "\n",
      "Predicted prefix:\n",
      " DAMAGE or\n",
      "for suffix:\n",
      " , Inc. or its affiliates. All Rights Reserved.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\").\n",
      " * You may not use this file except in\n",
      "Loss for suffix given predicted prefix is 0.8049339652061462 \n",
      " Suffix loss for true prefix is 0.23380467295646667\n",
      "NLL on predicted prefix is 6.734943866729736 \n",
      " NLL on true prefix is 2.0527138710021973\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jp6263/reverse-dynamics-nlp/jupyworkshop.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvscode~greene/home/jp6263/reverse-dynamics-nlp/jupyworkshop.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m len_prefix \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer(prefix)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvscode~greene/home/jp6263/reverse-dynamics-nlp/jupyworkshop.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m rand_prefix \u001b[39m=\u001b[39m rand_init(len_prefix, tokenizer)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvscode~greene/home/jp6263/reverse-dynamics-nlp/jupyworkshop.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m optimized_string \u001b[39m=\u001b[39m gcg\u001b[39m.\u001b[39;49moptimize(rand_prefix, suffix)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvscode~greene/home/jp6263/reverse-dynamics-nlp/jupyworkshop.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m optimal_prefix \u001b[39m=\u001b[39m optimized_string[:len_prefix]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvscode~greene/home/jp6263/reverse-dynamics-nlp/jupyworkshop.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m predicted_prefix_loss, predicted_suffix_loss \u001b[39m=\u001b[39m forward_loss(model, (optimal_prefix, suffix))\n",
      "File \u001b[0;32m~/reverse-dynamics-nlp/prompt_optimizer.py:89\u001b[0m, in \u001b[0;36mPromptOptimizer.optimize\u001b[0;34m(self, initial_input, target_string, use_prefix_loss)\u001b[0m\n\u001b[1;32m     87\u001b[0m prop_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(proposals)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     88\u001b[0m targets \u001b[39m=\u001b[39m input_ids[target_slice]\n\u001b[0;32m---> 89\u001b[0m losses \u001b[39m=\u001b[39m [nn\u001b[39m.\u001b[39mCrossEntropyLoss()(prop_logits[pidx, loss_slice, :], targets)\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m pidx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(prop_logits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]\n\u001b[1;32m     90\u001b[0m \u001b[39m# Add a penalty for unlikely prompts that are not very high-likelihood\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m use_prefix_loss:\n",
      "File \u001b[0;32m~/reverse-dynamics-nlp/prompt_optimizer.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m prop_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(proposals)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     88\u001b[0m targets \u001b[39m=\u001b[39m input_ids[target_slice]\n\u001b[0;32m---> 89\u001b[0m losses \u001b[39m=\u001b[39m [nn\u001b[39m.\u001b[39;49mCrossEntropyLoss()(prop_logits[pidx, loss_slice, :], targets)\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m pidx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(prop_logits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]\n\u001b[1;32m     90\u001b[0m \u001b[39m# Add a penalty for unlikely prompts that are not very high-likelihood\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m use_prefix_loss:\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/slacktokens/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/slacktokens/lib/python3.8/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/slacktokens/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## OPTIMAL STRING MAY NOT BE TRUNCATED TO CORRECT PREFIX LENGTH\n",
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if p==5: break\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    rand_prefix = rand_init(len_prefix, tokenizer)\n",
    "    optimized_string = gcg.optimize(rand_prefix, suffix)\n",
    "    optimal_prefix = optimized_string[:len_prefix]\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (optimal_prefix, suffix))\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{optimal_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "    # tokenwise_acc.append(sum([1 for i in range(len(prefix)) if prefix[i] == optimal_prefix[i]])/len(prefix))\n",
    "print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "                                                                                  [PUBLISH]\n",
      "\n",
      "\n",
      "                   \n",
      "\n",
      "Predicted prefix:\n",
      "           [DO NOT PUBLISH]\n",
      "\n",
      "\n",
      "                IN\n",
      "for suffix:\n",
      " IN THE UNITED STATES COURT OF APPEALS\n",
      "\n",
      "                            FOR THE ELEVENTH CIRC\n",
      "Loss for suffix given predicted prefix is 1.5578012466430664 \n",
      " Suffix loss for true prefix is 1.1715296506881714\n",
      "NLL on predicted prefix is 5.3471269607543945 \n",
      " NLL on true prefix is 3.7086830139160156\n",
      "True prefix is:\n",
      "Mystikal (album)\n",
      "\n",
      "My \n",
      "\n",
      "Predicted prefix:\n",
      " he said.Mystikal\n",
      "\n",
      "My\n",
      "for suffix:\n",
      " stikal is the eponymous self-titled debut studio album by American rapper Mystikal. It was independently self-released on June 14, 1994, by Big Boy Records. The\n",
      "Loss for suffix given predicted prefix is 2.3714146614074707 \n",
      " Suffix loss for true prefix is 2.054072141647339\n",
      "NLL on predicted prefix is 4.269046783447266 \n",
      " NLL on true prefix is 3.6076202392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "5. Let b be (-105)/(-10)* \n",
      "\n",
      "Predicted prefix:\n",
      "*k. Let b be 4/2*\n",
      "for suffix:\n",
      " 6/k. Let y be (-3)/1 + (-196)/b. Solve -y = 4*u + 3*v, -2*v + 5*v = -2\n",
      "Loss for suffix given predicted prefix is 1.522289514541626 \n",
      " Suffix loss for true prefix is 1.646061658859253\n",
      "NLL on predicted prefix is 2.721585750579834 \n",
      " NLL on true prefix is 3.1036183834075928\n",
      "True prefix is:\n",
      "Hyperolius ferrugineus\n",
      "\n",
      "Hyper \n",
      "\n",
      "Predicted prefix:\n",
      "Hyperolius ferrugineus\n",
      "\n",
      "Hyper\n",
      "for suffix:\n",
      " olius ferrugineus is a species of frog in the family Hyperoliidae.\n",
      "It is endemic to Democratic Republic of the Congo.\n",
      "Its natural habitats are subtropical or tropical mois\n",
      "Loss for suffix given predicted prefix is 1.979470133781433 \n",
      " Suffix loss for true prefix is 1.979470133781433\n",
      "NLL on predicted prefix is 4.303708553314209 \n",
      " NLL on true prefix is 4.303708553314209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "Hatamabad, Markazi\n",
      "\n",
      "H \n",
      "\n",
      "Predicted prefix:\n",
      " he said.Jatamabad\n",
      "\n",
      "J\n",
      "for suffix:\n",
      " atamabad (, also Romanized as Ḩātamābād) is a village in Farmahin Rural District, in the Central District of Farahan County, Markazi\n",
      "Loss for suffix given predicted prefix is 1.5585622787475586 \n",
      " Suffix loss for true prefix is 1.4432446956634521\n",
      "NLL on predicted prefix is 4.541042804718018 \n",
      " NLL on true prefix is 4.4801740646362305\n",
      "True prefix is:\n",
      "<?php defined('BX_DOL') \n",
      "\n",
      "Predicted prefix:\n",
      "_BEFORE_HACK_PAUSE')\n",
      "for suffix:\n",
      "  or die('hack attempt');\n",
      "/**\n",
      " * Copyright (c) UNA, Inc - https://una.io\n",
      " * MIT License - https://opensource.org/licenses/MIT\n",
      "Loss for suffix given predicted prefix is 1.8252325057983398 \n",
      " Suffix loss for true prefix is 1.6047663688659668\n",
      "NLL on predicted prefix is 4.435699462890625 \n",
      " NLL on true prefix is 4.108913421630859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "<header class=\"header-wrapper\">\n",
      "\n",
      "   \n",
      "\n",
      "Predicted prefix:\n",
      ">\n",
      "<div class=\"container\">\n",
      "  \n",
      "for suffix:\n",
      " <nav class=\"inner\">\n",
      "    <div class=\"title\">\n",
      "      <a href=\"/\">\n",
      "        <img class=\"logo\" src=\"<%- url_for(theme.profile\n",
      "Loss for suffix given predicted prefix is 1.8682317733764648 \n",
      " Suffix loss for true prefix is 1.8162084817886353\n",
      "NLL on predicted prefix is 1.1058136224746704 \n",
      " NLL on true prefix is 2.780362367630005\n",
      "True prefix is:\n",
      "/*\n",
      "Copyright (C) 2011 Mark Chandler ( \n",
      "\n",
      "Predicted prefix:\n",
      " part of the Desura(R) project (\n",
      "for suffix:\n",
      " Desura Net Pty Ltd)\n",
      "Copyright (C) 2014 Bad Juju Games, Inc.\n",
      "\n",
      "This program is free software: you can redistribute it and/or modify\n",
      "it under the\n",
      "Loss for suffix given predicted prefix is 2.050963878631592 \n",
      " Suffix loss for true prefix is 1.5706523656845093\n",
      "NLL on predicted prefix is 4.815761566162109 \n",
      " NLL on true prefix is 4.146985054016113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "The present invention relates to catalyst components for the polymerization \n",
      "\n",
      "Predicted prefix:\n",
      ", to the use of said catalysts in the polymerization\n",
      "for suffix:\n",
      "  of olefins, to the catalysts obtained therefrom and to the use of said catalysts in the polymerization of olefins CH2xe2x95x\n",
      "Loss for suffix given predicted prefix is 1.7928210496902466 \n",
      " Suffix loss for true prefix is 1.878375768661499\n",
      "NLL on predicted prefix is 3.803551435470581 \n",
      " NLL on true prefix is 2.978477954864502\n",
      "True prefix is:\n",
      "1. Introduction {#sec1-ijerph-17 \n",
      "\n",
      "Predicted prefix:\n",
      "1. Introduction {#sec1-ijerph-17\n",
      "for suffix:\n",
      " -01067}\n",
      "===============\n",
      "\n",
      "Nasolacrimal duct obstruction (NLDO) is the most common cause of childhood epiphora \\[[@B1-ijerph-17-010\n",
      "Loss for suffix given predicted prefix is 1.7206493616104126 \n",
      " Suffix loss for true prefix is 1.7206493616104126\n",
      "NLL on predicted prefix is 1.8608165979385376 \n",
      " NLL on true prefix is 1.8608165979385376\n",
      "Average loss is 1.824743640422821\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==100: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    predicted_prefix = reverse_generate(bwd_model, tokenizer, suffix, len_prefix)[0]\n",
    "    predicted_prefix = tokenizer.decode(tokenizer.encode(predicted_prefix)[:len_prefix])\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reverse_tokenize, reverse_decode\n",
    "def reverse_normalized_forward(reverse_model, tokenizer, target, normalizer):\n",
    "    inputs = reverse_tokenize(tokenizer, target)\n",
    "    outputs = reverse_model(inputs).logits[0,-1,:]\n",
    "    outputs = torch.nn.Softmax(dim=-1)(outputs).cpu()\n",
    "    outputs = torch.mul(outputs, normalizer)\n",
    "    return outputs\n",
    "\n",
    "def reverse_normalized_generate(reverse_model, tokenizer, target, max_length, normalizer, temperature=1):\n",
    "    prefix = []\n",
    "    for i in range(max_length):\n",
    "        normalized_probs = reverse_normalized_forward(reverse_model, tokenizer, ''.join(prefix[::-1]) + target, normalizer)\n",
    "        if not temperature:\n",
    "            token = tokenizer.decode(torch.argmax(normalized_probs))\n",
    "        else:\n",
    "            probs = torch.div(normalized_probs, temperature)\n",
    "            probs = torch.nn.Softmax(dim=-1)(probs)\n",
    "            token = tokenizer.decode(torch.multinomial(probs, num_samples=1))\n",
    "        if token == '[PAD]' or token == '[EOS]':\n",
    "            break\n",
    "        prefix.append(token)\n",
    "    return ''.join(prefix[::-1])+target\n",
    "\n",
    "inverse_dataset_probs = torch.reciprocal(dataset_probs)\n",
    "reverse_normalized_generate(bwd_model, tokenizer, ' on the mat next to the kitchen.', 5, inverse_dataset_probs**0.1, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss is 1.5034217107743024\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==250: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, inverse_dataset_probs**0.25, temperature=0) #1.425 at 0.25 partial Bayes update vs 1.437 at 0 i.e. default\n",
    "    predicted_prefix = tokenizer.decode(tokenizer.encode(predicted_prefix)[:len_prefix])\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\n')\n",
    "    # print(f'Predicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jp6263/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 508.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_token_probabilities(tokenizer, dataset=\"NeelNanda/pile-10k\", vocab_size=50304):\n",
    "    data = load_dataset(dataset)\n",
    "    counts = torch.zeros(vocab_size, dtype=torch.float) #tokenizer.vocab_size is fake 50304 is the model output dimension which is what we care about\n",
    "\n",
    "    for chunk in data['train']:\n",
    "        # Extract text from chunk (assuming each chunk is a dictionary with a \"text\" key)\n",
    "        text = chunk['text']\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "        # Count occurrences for each token\n",
    "        for tok in tokens:\n",
    "            counts[tok] += 1\n",
    "\n",
    "    # Normalize the counts to get probabilities\n",
    "    total_tokens = torch.sum(counts)\n",
    "    probabilities = counts / total_tokens\n",
    "    min_val = probabilities[probabilities > 0].min()\n",
    "    probabilities[probabilities == 0] = min_val\n",
    "    return probabilities\n",
    "\n",
    "dataset_probs = get_token_probabilities(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' it, and set it on the mat next to the kitchen.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_generate(bwd_model, tokenizer, ' on the mat next to the kitchen.', 5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slacktokens",
   "language": "python",
   "name": "slacktokens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
