{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/slacktokens/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_list = os.chdir('./../reverse-dynamics-nlp/')\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM\n",
    "from prompt_optimizer import PromptOptimizer\n",
    "from utils import reverse_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Any\n",
    "def get_reverse_pair(dataset: Iterable[Any], chunk_func: Callable[..., Any], tokenizer: AutoTokenizer):\n",
    "    for chunk in dataset:\n",
    "        for chunk in chunk_func(chunk, tokenizer):\n",
    "            yield chunk\n",
    "\n",
    "def end_chunk_hf(chunk, tokenizer):\n",
    "    chunk = chunk['text']\n",
    "    tokens = tokenizer(chunk[-200:])['input_ids'][2:] #drop first couple tokens given risk of incomplete token\n",
    "    yield tokenizer.decode(tokens[-40:-30]), tokenizer.decode(tokens[-30:])\n",
    "\n",
    "def start_chunk_hf(chunk, tokenizer, num_prefix_tokens=10, num_suffix_tokens=40):\n",
    "    chunk = chunk['text']\n",
    "    tokens = tokenizer(chunk[:200])['input_ids'] #drop first couple tokens given risk of incomplete token\n",
    "    yield tokenizer.decode(tokens[:num_prefix_tokens]), tokenizer.decode(tokens[num_prefix_tokens:num_prefix_tokens+num_suffix_tokens])\n",
    "\n",
    "def rand_init(seq_length: int, tokenizer):\n",
    "    return tokenizer.decode(torch.randint(0, tokenizer.vocab_size, (seq_length,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jp6263/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 433.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It is done, and submitted. You can play', ' “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"NeelNanda/pile-10k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"afterless/reverse-pythia-160m\")\n",
    "pairs = get_reverse_pair(dataset['train'], start_chunk_hf, tokenizer)\n",
    "print(next(pairs))\n",
    "bwd_model = GPTNeoXForCausalLM.from_pretrained(\"afterless/reverse-pythia-160m\").cuda()\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\", cache_dir='/scratch/jp6263/hf/models/').cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(model, pair, loss=torch.nn.CrossEntropyLoss(),):\n",
    "    prefix, suffix = pair\n",
    "    whole_tensor = tokenizer(prefix+suffix, return_tensors='pt').input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        logs = model(whole_tensor).logits\n",
    "    start_ind = len(tokenizer.encode(prefix))\n",
    "    l_pref = loss(logs[0,:start_ind], whole_tensor[0,1:start_ind+1])\n",
    "    l_suff = loss(logs[0,start_ind:-1], whole_tensor[0,start_ind+1:])\n",
    "    return l_pref, l_suff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcg = PromptOptimizer(model, tokenizer, n_proposals=128, n_epochs=250, n_top_indices=128, prefix_loss_weight=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "<?xml version=\"1.0\" encoding=\" \n",
      "\n",
      "Predicted prefix:\n",
      " Zend_, prostate missionary}$$\\ige ${{\\waitSeg?\"\n",
      "for suffix:\n",
      " UTF-8\"?>\n",
      "<segment>\n",
      "    <name>PD1</name>\n",
      "    <description>Patient Additional Demographic</description>\n",
      "    <elements>\n",
      "        <field\n",
      "Loss for suffix given predicted prefix is 1.967311143875122 \n",
      " Suffix loss for true prefix is 2.0244696140289307\n",
      "NLL on predicted prefix is 12.9093599319458 \n",
      " NLL on true prefix is 0.6212190389633179\n",
      "True prefix is:\n",
      "{\n",
      "  \"fpsLimit\": 60, \n",
      "\n",
      "Predicted prefix:\n",
      "dialopacity bilWVeveJSON ripownerOFFSET collagen\n",
      "for suffix:\n",
      " \n",
      "  \"preset\": \"basic\",\n",
      "  \"background\": {\n",
      "    \"color\": \"#0d47a1\",\n",
      "    \"image\": \"\",\n",
      "    \"position\": \"50\n",
      "Loss for suffix given predicted prefix is 2.122087240219116 \n",
      " Suffix loss for true prefix is 1.8352282047271729\n",
      "NLL on predicted prefix is 12.266073226928711 \n",
      " NLL on true prefix is 3.9634578227996826\n",
      "True prefix is:\n",
      "This application is based upon and claims the benefit of \n",
      "\n",
      "Predicted prefix:\n",
      "iticMeasureManagement outstandinglisted;QSto representsclaim\n",
      "for suffix:\n",
      "  priority from the prior Japanese Patent Application No. 2000-159163, filed Mar. 31, 2000, the entire contents of which are incorporated herein \n",
      "Loss for suffix given predicted prefix is 1.7409389019012451 \n",
      " Suffix loss for true prefix is 1.6886787414550781\n",
      "NLL on predicted prefix is 10.64742374420166 \n",
      " NLL on true prefix is 2.7436904907226562\n",
      "True prefix is:\n",
      "     The summaries of the Colorado Court of Appeals \n",
      "\n",
      "Predicted prefix:\n",
      "recipomesrloan equations Dept Nielsen COURT Orthographically\n",
      "for suffix:\n",
      "  published opinions\n",
      "  constitute no part of the opinion of the division but have been prepared by\n",
      "  the division for the convenience of the reader. T\n",
      "Loss for suffix given predicted prefix is 1.676066279411316 \n",
      " Suffix loss for true prefix is 1.7980828285217285\n",
      "NLL on predicted prefix is 10.337575912475586 \n",
      " NLL on true prefix is 4.218183994293213\n",
      "True prefix is:\n",
      "---------------------- Forwarded by Benjamin Rogers/HOU/ECT \n",
      "\n",
      "Predicted prefix:\n",
      "bserver800===== raisedHOU dismissingθostonenron.\n",
      "for suffix:\n",
      "  on 10/19/2000 \n",
      "03:13 PM ---------------------------\n",
      "\n",
      "\n",
      "Dplflan@aol.com on 10/18/2000 06:18:51 PM\n",
      "To: Benjamin\n",
      "Loss for suffix given predicted prefix is 2.16984486579895 \n",
      " Suffix loss for true prefix is 2.014803886413574\n",
      "NLL on predicted prefix is 10.853466033935547 \n",
      " NLL on true prefix is 3.4035935401916504\n",
      "Average loss is 1.93524968624115\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "temp = 5 #None for default GCG with uniform sampling\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==5: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    rand_prefix = rand_init(len_prefix, tokenizer)\n",
    "    optimized_string = gcg.optimize(rand_prefix, suffix, temperature=temp)\n",
    "    optimal_prefix = tokenizer.decode(tokenizer.encode(optimized_string)[:len_prefix])\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (optimal_prefix, suffix))\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{optimal_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "    # tokenwise_acc.append(sum([1 for i in range(len(prefix)) if prefix[i] == optimal_prefix[i]])/len(prefix))\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "                                                                                  [PUBLISH]\n",
      "\n",
      "\n",
      "                   \n",
      "\n",
      "Predicted prefix:\n",
      "           [DO NOT PUBLISH]\n",
      "\n",
      "\n",
      "                IN\n",
      "for suffix:\n",
      " IN THE UNITED STATES COURT OF APPEALS\n",
      "\n",
      "                            FOR THE ELEVENTH CIRC\n",
      "Loss for suffix given predicted prefix is 1.5578012466430664 \n",
      " Suffix loss for true prefix is 1.1715296506881714\n",
      "NLL on predicted prefix is 5.3471269607543945 \n",
      " NLL on true prefix is 3.7086830139160156\n",
      "True prefix is:\n",
      "Mystikal (album)\n",
      "\n",
      "My \n",
      "\n",
      "Predicted prefix:\n",
      " he said.Mystikal\n",
      "\n",
      "My\n",
      "for suffix:\n",
      " stikal is the eponymous self-titled debut studio album by American rapper Mystikal. It was independently self-released on June 14, 1994, by Big Boy Records. The\n",
      "Loss for suffix given predicted prefix is 2.3714146614074707 \n",
      " Suffix loss for true prefix is 2.054072141647339\n",
      "NLL on predicted prefix is 4.269046783447266 \n",
      " NLL on true prefix is 3.6076202392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "5. Let b be (-105)/(-10)* \n",
      "\n",
      "Predicted prefix:\n",
      "*k. Let b be 4/2*\n",
      "for suffix:\n",
      " 6/k. Let y be (-3)/1 + (-196)/b. Solve -y = 4*u + 3*v, -2*v + 5*v = -2\n",
      "Loss for suffix given predicted prefix is 1.522289514541626 \n",
      " Suffix loss for true prefix is 1.646061658859253\n",
      "NLL on predicted prefix is 2.721585750579834 \n",
      " NLL on true prefix is 3.1036183834075928\n",
      "True prefix is:\n",
      "Hyperolius ferrugineus\n",
      "\n",
      "Hyper \n",
      "\n",
      "Predicted prefix:\n",
      "Hyperolius ferrugineus\n",
      "\n",
      "Hyper\n",
      "for suffix:\n",
      " olius ferrugineus is a species of frog in the family Hyperoliidae.\n",
      "It is endemic to Democratic Republic of the Congo.\n",
      "Its natural habitats are subtropical or tropical mois\n",
      "Loss for suffix given predicted prefix is 1.979470133781433 \n",
      " Suffix loss for true prefix is 1.979470133781433\n",
      "NLL on predicted prefix is 4.303708553314209 \n",
      " NLL on true prefix is 4.303708553314209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "Hatamabad, Markazi\n",
      "\n",
      "H \n",
      "\n",
      "Predicted prefix:\n",
      " he said.Jatamabad\n",
      "\n",
      "J\n",
      "for suffix:\n",
      " atamabad (, also Romanized as Ḩātamābād) is a village in Farmahin Rural District, in the Central District of Farahan County, Markazi\n",
      "Loss for suffix given predicted prefix is 1.5585622787475586 \n",
      " Suffix loss for true prefix is 1.4432446956634521\n",
      "NLL on predicted prefix is 4.541042804718018 \n",
      " NLL on true prefix is 4.4801740646362305\n",
      "True prefix is:\n",
      "<?php defined('BX_DOL') \n",
      "\n",
      "Predicted prefix:\n",
      "_BEFORE_HACK_PAUSE')\n",
      "for suffix:\n",
      "  or die('hack attempt');\n",
      "/**\n",
      " * Copyright (c) UNA, Inc - https://una.io\n",
      " * MIT License - https://opensource.org/licenses/MIT\n",
      "Loss for suffix given predicted prefix is 1.8252325057983398 \n",
      " Suffix loss for true prefix is 1.6047663688659668\n",
      "NLL on predicted prefix is 4.435699462890625 \n",
      " NLL on true prefix is 4.108913421630859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "<header class=\"header-wrapper\">\n",
      "\n",
      "   \n",
      "\n",
      "Predicted prefix:\n",
      ">\n",
      "<div class=\"container\">\n",
      "  \n",
      "for suffix:\n",
      " <nav class=\"inner\">\n",
      "    <div class=\"title\">\n",
      "      <a href=\"/\">\n",
      "        <img class=\"logo\" src=\"<%- url_for(theme.profile\n",
      "Loss for suffix given predicted prefix is 1.8682317733764648 \n",
      " Suffix loss for true prefix is 1.8162084817886353\n",
      "NLL on predicted prefix is 1.1058136224746704 \n",
      " NLL on true prefix is 2.780362367630005\n",
      "True prefix is:\n",
      "/*\n",
      "Copyright (C) 2011 Mark Chandler ( \n",
      "\n",
      "Predicted prefix:\n",
      " part of the Desura(R) project (\n",
      "for suffix:\n",
      " Desura Net Pty Ltd)\n",
      "Copyright (C) 2014 Bad Juju Games, Inc.\n",
      "\n",
      "This program is free software: you can redistribute it and/or modify\n",
      "it under the\n",
      "Loss for suffix given predicted prefix is 2.050963878631592 \n",
      " Suffix loss for true prefix is 1.5706523656845093\n",
      "NLL on predicted prefix is 4.815761566162109 \n",
      " NLL on true prefix is 4.146985054016113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True prefix is:\n",
      "The present invention relates to catalyst components for the polymerization \n",
      "\n",
      "Predicted prefix:\n",
      ", to the use of said catalysts in the polymerization\n",
      "for suffix:\n",
      "  of olefins, to the catalysts obtained therefrom and to the use of said catalysts in the polymerization of olefins CH2xe2x95x\n",
      "Loss for suffix given predicted prefix is 1.7928210496902466 \n",
      " Suffix loss for true prefix is 1.878375768661499\n",
      "NLL on predicted prefix is 3.803551435470581 \n",
      " NLL on true prefix is 2.978477954864502\n",
      "True prefix is:\n",
      "1. Introduction {#sec1-ijerph-17 \n",
      "\n",
      "Predicted prefix:\n",
      "1. Introduction {#sec1-ijerph-17\n",
      "for suffix:\n",
      " -01067}\n",
      "===============\n",
      "\n",
      "Nasolacrimal duct obstruction (NLDO) is the most common cause of childhood epiphora \\[[@B1-ijerph-17-010\n",
      "Loss for suffix given predicted prefix is 1.7206493616104126 \n",
      " Suffix loss for true prefix is 1.7206493616104126\n",
      "NLL on predicted prefix is 1.8608165979385376 \n",
      " NLL on true prefix is 1.8608165979385376\n",
      "Average loss is 1.824743640422821\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==100: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    predicted_prefix = reverse_generate(bwd_model, tokenizer, suffix, len_prefix)[0]\n",
    "    predicted_prefix = tokenizer.decode(tokenizer.encode(predicted_prefix)[:len_prefix])\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    print(f'True prefix is:\\n{prefix} \\n\\nPredicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reverse_tokenize, reverse_decode\n",
    "def reverse_normalized_forward(reverse_model, tokenizer, target, normalizer):\n",
    "    inputs = reverse_tokenize(tokenizer, target)\n",
    "    outputs = reverse_model(inputs).logits[0,-1,:]\n",
    "    outputs = torch.nn.Softmax(dim=-1)(outputs).cpu()\n",
    "    outputs = torch.mul(outputs, normalizer)\n",
    "    return outputs\n",
    "\n",
    "def reverse_normalized_generate(reverse_model, tokenizer, target, max_length, normalizer, temperature=1):\n",
    "    prefix = []\n",
    "    for i in range(max_length):\n",
    "        normalized_probs = reverse_normalized_forward(reverse_model, tokenizer, ''.join(prefix[::-1]) + target, normalizer)\n",
    "        if not temperature:\n",
    "            token = tokenizer.decode(torch.argmax(normalized_probs))\n",
    "        else:\n",
    "            probs = torch.div(normalized_probs, temperature)\n",
    "            probs = torch.nn.Softmax(dim=-1)(probs)\n",
    "            token = tokenizer.decode(torch.multinomial(probs, num_samples=1))\n",
    "        if token == '[PAD]' or token == '[EOS]':\n",
    "            break\n",
    "        prefix.append(token)\n",
    "    return ''.join(prefix[::-1])+target\n",
    "\n",
    "inverse_dataset_probs = torch.reciprocal(dataset_probs)\n",
    "reverse_normalized_generate(bwd_model, tokenizer, ' on the mat next to the kitchen.', 5, inverse_dataset_probs**0.1, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss is 1.5034217107743024\n"
     ]
    }
   ],
   "source": [
    "tokenwise_acc = []\n",
    "loss = []\n",
    "for p,pair in enumerate(pairs):\n",
    "    if len(loss)==250: break\n",
    "    if len(pair[0])<10 or len(pair[1])<10: continue\n",
    "    prefix_loss,suffix_loss = forward_loss(model, pair)\n",
    "    # if suffix_loss>2.1: continue #this is around 10th percentile of losses for 170m\n",
    "    prefix, suffix = pair\n",
    "    len_prefix = len(tokenizer(prefix)['input_ids'])\n",
    "    predicted_prefix = reverse_normalized_generate(bwd_model, tokenizer, suffix, len_prefix, inverse_dataset_probs**0.25, temperature=0) #1.425 at 0.25 partial Bayes update vs 1.437 at 0 i.e. default\n",
    "    predicted_prefix = tokenizer.decode(tokenizer.encode(predicted_prefix)[:len_prefix])\n",
    "\n",
    "    predicted_prefix_loss, predicted_suffix_loss = forward_loss(model, (predicted_prefix, suffix))\n",
    "    # print(f'True prefix is:\\n{prefix} \\n\\n')\n",
    "    # print(f'Predicted prefix:\\n{predicted_prefix}\\nfor suffix:\\n {suffix}')\n",
    "    # print(f'Loss for suffix given predicted prefix is {predicted_suffix_loss.item()} \\n Suffix loss for true prefix is {suffix_loss.item()}')\n",
    "    # print(f'NLL on predicted prefix is {predicted_prefix_loss.item()} \\n NLL on true prefix is {prefix_loss.item()}')\n",
    "    loss.append(predicted_suffix_loss.item())\n",
    "# print(f'Average tokenwise accuracy is {sum(tokenwise_acc)/len(tokenwise_acc)}')\n",
    "print(f'Average loss is {sum(loss)/len(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jp6263/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 508.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_token_probabilities(tokenizer, dataset=\"NeelNanda/pile-10k\", vocab_size=50304):\n",
    "    data = load_dataset(dataset)\n",
    "    counts = torch.zeros(vocab_size, dtype=torch.float) #tokenizer.vocab_size is fake 50304 is the model output dimension which is what we care about\n",
    "\n",
    "    for chunk in data['train']:\n",
    "        # Extract text from chunk (assuming each chunk is a dictionary with a \"text\" key)\n",
    "        text = chunk['text']\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "        # Count occurrences for each token\n",
    "        for tok in tokens:\n",
    "            counts[tok] += 1\n",
    "\n",
    "    # Normalize the counts to get probabilities\n",
    "    total_tokens = torch.sum(counts)\n",
    "    probabilities = counts / total_tokens\n",
    "    min_val = probabilities[probabilities > 0].min()\n",
    "    probabilities[probabilities == 0] = min_val\n",
    "    return probabilities\n",
    "\n",
    "dataset_probs = get_token_probabilities(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' it, and set it on the mat next to the kitchen.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_generate(bwd_model, tokenizer, ' on the mat next to the kitchen.', 5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slacktokens",
   "language": "python",
   "name": "slacktokens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
