{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    revision=\"step3000\",\n",
    ").to(device)\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "reverse_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    \"afterless/reverse-pythia-160m\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to test whether the get_cond_logprob and log_prob is in fact working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_logprob_old(input_ids, model):\n",
    "    # Get conditional logprobs\n",
    "    with torch.no_grad():\n",
    "        logprobs = torch.nn.functional.log_softmax(\n",
    "            model(input_ids=input_ids).logits, dim=-1\n",
    "        )\n",
    "    # Get the log probabilities corresponding to the words in input_ids\n",
    "    relevant_logprobs = torch.gather(\n",
    "        logprobs, 2, input_ids.unsqueeze(-1)[:, 1:]\n",
    "    ).squeeze(-1)\n",
    "    # Sum log probabilities over the sequence length dimension\n",
    "    sum_log_probs = relevant_logprobs.sum(dim=1)\n",
    "    return sum_log_probs\n",
    "\n",
    "def get_cond_logprob_test(input_ids, model):\n",
    "    # Get conditional logprobs\n",
    "    with torch.no_grad():\n",
    "        logprobs = torch.nn.functional.log_softmax(\n",
    "            model(input_ids=input_ids[:,:-1]).logits, dim=-1\n",
    "        )\n",
    "    # Get the log probabilities corresponding to the words in input_ids\n",
    "    relevant_logprobs = torch.gather(\n",
    "        logprobs, 2, input_ids.unsqueeze(-1)[:, 1:]\n",
    "    ).squeeze(-1)\n",
    "    # Sum log probabilities over the sequence length dimension\n",
    "    sum_log_probs = relevant_logprobs.sum(dim=1)\n",
    "    return sum_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-13.3137])\n",
      "tensor([-13.3137])\n"
     ]
    }
   ],
   "source": [
    "suffix = \"Yoooooo\"\n",
    "tokenized_suffix = tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "print(get_cond_logprob_old(tokenized_suffix, model))\n",
    "print(get_cond_logprob_test(tokenized_suffix, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logprobs = torch.nn.functional.log_softmax(\n",
    "        model(input_ids=tokenized_suffix).logits, dim=-1\n",
    "    )\n",
    "# Get the log probabilities corresponding to the words in input_ids\n",
    "relevant_logprobs = torch.gather(\n",
    "    logprobs, 2, tokenized_suffix.unsqueeze(-1)[:, 1:]\n",
    ").squeeze(-1)\n",
    "# Sum log probabilities over the sequence length dimension\n",
    "sum_log_probs = relevant_logprobs.sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_suffix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50304])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_suffix.unsqueeze(-1)[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_logprobs = torch.gather(\n",
    "    logprobs, 2, tokenized_suffix.unsqueeze(-1)[:, 1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[1, 2], [3, 4]])\n",
    "# a = torch.gather(t, 0, torch.tensor([[0, 0], [1, 0]]))\n",
    "index = torch.tensor([[0,0]])\n",
    "out  = torch.gather(input, 1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0, 0], [1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 145.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "test = create_chunked_dataset_from_full_sequences(\n",
    "        \"pile_val\",\n",
    "        tokenizer,\n",
    "        10,\n",
    "        2048,\n",
    "        suffix_length=1,\n",
    "        batch_size=1,\n",
    "        seed=23,\n",
    "        return_all = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6729]])\n"
     ]
    }
   ],
   "source": [
    "prefix_length = 1\n",
    "suffix = \" Obama\"\n",
    "tokenized_suffix = tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "print(tokenized_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9799)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_suffix = tokenized_suffix.flip(dims=[1]).to(device)        \n",
    "            \n",
    "reverse_input_ids = reverse_suffix[:, :-1]\n",
    "reverse_targets = reverse_suffix[:, 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "  reverse_outputs = reverse_model(input_ids=reverse_input_ids)\n",
    "  reverse_logits = reverse_outputs.logits\n",
    "\n",
    "torch.nn.CrossEntropyLoss()(reverse_logits.squeeze(0), reverse_targets.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6060)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenized_suffix[:,0:-1]\n",
    "targets = tokenized_suffix[:, 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(input_ids=input_ids)\n",
    "  logits = outputs.logits\n",
    "\n",
    "\n",
    "torch.nn.CrossEntropyLoss()(logits.squeeze(0), targets.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.ones(1,6000, dtype=torch.long).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  model_outputs = model(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4000, 50304])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverse_sampling import sample_reverse_dynamics\n",
    "test_suffix = torch.ones((1, 1), dtype=torch.long).to(device)\n",
    "output1, logits1 = sample_reverse_dynamics(\n",
    "    model,\n",
    "    empirical_dist,\n",
    "    prefix_length,\n",
    "    test_suffix,\n",
    "    vocab_batch_size=1000,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [00:18<00:00, 21.33it/s]\n",
      "100%|██████████| 393/393 [00:20<00:00, 19.16it/s]\n",
      "100%|██████████| 393/393 [00:27<00:00, 14.46it/s]\n",
      "100%|██████████| 393/393 [00:32<00:00, 12.13it/s]\n",
      "100%|██████████| 393/393 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 393/393 [00:41<00:00,  9.38it/s]\n",
      "100%|██████████| 393/393 [00:45<00:00,  8.64it/s]\n",
      "100%|██████████| 393/393 [00:33<00:00, 11.67it/s]\n",
      "100%|██████████| 393/393 [00:34<00:00, 11.53it/s]\n",
      "100%|██████████| 393/393 [01:04<00:00,  6.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from reverse_sampling import sample_reverse_dynamics_reverse_prior\n",
    "\n",
    "output1, logits1 = sample_reverse_dynamics_reverse_prior(\n",
    "    model,\n",
    "    reverse_model,\n",
    "    prefix_length,\n",
    "    test_suffix,\n",
    "    vocab_batch_size=300,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix = \" President Donald Trump filed a lawsuit against former President Barack Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "tokenized_suffix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/393 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m suffix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m President Donald Trump filed a lawsuit against former President Barack Obama\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenized_suffix\u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(suffix, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m compute_loss_reverse_dynamics_reverse_prior(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     reverse_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     tokenized_suffix,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     vocab_batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     device \u001b[39m=\u001b[39;49m device\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:197\u001b[0m, in \u001b[0;36mcompute_loss_reverse_dynamics_reverse_prior\u001b[0;34m(model, reverse_model, tokenized_suffix, vocab_batch_size, dilution, device, loss)\u001b[0m\n\u001b[1;32m    194\u001b[0m     uniform_dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(prior_dist) \u001b[39m/\u001b[39m prior_dist\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    195\u001b[0m     prior_dist \u001b[39m=\u001b[39m prior_dist \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mdilution) \u001b[39m+\u001b[39m uniform_dist \u001b[39m*\u001b[39m dilution\n\u001b[0;32m--> 197\u001b[0m     logits \u001b[39m=\u001b[39m compute_posterior(\n\u001b[1;32m    198\u001b[0m         model,\n\u001b[1;32m    199\u001b[0m         prior_dist,\n\u001b[1;32m    200\u001b[0m         splus,\n\u001b[1;32m    201\u001b[0m         vocab_batch_size,\n\u001b[1;32m    202\u001b[0m         device\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m     full_logits \u001b[39m=\u001b[39m [logits,] \u001b[39m+\u001b[39m full_logits\n\u001b[1;32m    206\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(full_logits)\u001b[39m.\u001b[39mto(tokenized_suffix\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:55\u001b[0m, in \u001b[0;36mcompute_posterior\u001b[0;34m(model, stationary_dist, tokenized_suffix, vocab_batch_size, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m     batch_indices \u001b[39m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m         torch\u001b[39m.\u001b[39marange(start_idx, end_idx)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m     v_sentences \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     51\u001b[0m         (batch_indices\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), tokenized_suffix\u001b[39m.\u001b[39mrepeat(batch_indices\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)),\n\u001b[1;32m     52\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     53\u001b[0m     )\n\u001b[0;32m---> 55\u001b[0m     posterior\u001b[39m.\u001b[39mappend(get_logprob(v_sentences, model, stationary_dist))\n\u001b[1;32m     57\u001b[0m posterior \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(posterior)\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlog_softmax(posterior, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:26\u001b[0m, in \u001b[0;36mget_logprob\u001b[0;34m(input_ids, model, stationary_dist)\u001b[0m\n\u001b[1;32m     24\u001b[0m logprob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(stationary_dist[input_ids[:, \u001b[39m0\u001b[39m]])\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     logprob \u001b[39m=\u001b[39m logprob \u001b[39m+\u001b[39m get_cond_logprob(input_ids, model)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m logprob\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:12\u001b[0m, in \u001b[0;36mget_cond_logprob\u001b[0;34m(input_ids, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cond_logprob\u001b[39m(input_ids, model):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Get conditional logprobs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m         logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[0;32m---> 12\u001b[0m             model(input_ids\u001b[39m=\u001b[39;49minput_ids)\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Get the log probabilities corresponding to the words in input_ids\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     relevant_logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(\n\u001b[1;32m     16\u001b[0m         logprobs, \u001b[39m2\u001b[39m, input_ids\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39m1\u001b[39m:]\n\u001b[1;32m     17\u001b[0m     )\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:763\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 763\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    764\u001b[0m     input_ids,\n\u001b[1;32m    765\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    766\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    767\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    768\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    769\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    770\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    771\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    772\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    773\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    774\u001b[0m )\n\u001b[1;32m    776\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    777\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:619\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    616\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_in(input_ids)\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dropout(inputs_embeds)\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "from reverse_sampling import compute_loss_reverse_dynamics_reverse_prior\n",
    "\n",
    "\n",
    "suffix = \" President Donald Trump filed a lawsuit against former President Barack Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "\n",
    "loss = compute_loss_reverse_dynamics_reverse_prior(\n",
    "    model,\n",
    "    reverse_model,\n",
    "    tokenized_suffix,\n",
    "    vocab_batch_size=128,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/393 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/393 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m suffix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m The House Democrats oppose the impeachment of former President Barack Obama\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenized_suffix\u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(suffix, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loss \u001b[39m=\u001b[39m compute_loss_reverse_dynamics_reverse_prior(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     reverse_model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     tokenized_suffix,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     vocab_batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     device \u001b[39m=\u001b[39;49m device\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_dynamics_test.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:197\u001b[0m, in \u001b[0;36mcompute_loss_reverse_dynamics_reverse_prior\u001b[0;34m(model, reverse_model, tokenized_suffix, vocab_batch_size, dilution, device, loss)\u001b[0m\n\u001b[1;32m    194\u001b[0m     uniform_dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(prior_dist) \u001b[39m/\u001b[39m prior_dist\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    195\u001b[0m     prior_dist \u001b[39m=\u001b[39m prior_dist \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mdilution) \u001b[39m+\u001b[39m uniform_dist \u001b[39m*\u001b[39m dilution\n\u001b[0;32m--> 197\u001b[0m     logits \u001b[39m=\u001b[39m compute_posterior(\n\u001b[1;32m    198\u001b[0m         model,\n\u001b[1;32m    199\u001b[0m         prior_dist,\n\u001b[1;32m    200\u001b[0m         splus,\n\u001b[1;32m    201\u001b[0m         vocab_batch_size,\n\u001b[1;32m    202\u001b[0m         device\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m     full_logits \u001b[39m=\u001b[39m [logits,] \u001b[39m+\u001b[39m full_logits\n\u001b[1;32m    206\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(full_logits)\u001b[39m.\u001b[39mto(tokenized_suffix\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:55\u001b[0m, in \u001b[0;36mcompute_posterior\u001b[0;34m(model, stationary_dist, tokenized_suffix, vocab_batch_size, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m     batch_indices \u001b[39m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m         torch\u001b[39m.\u001b[39marange(start_idx, end_idx)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m     v_sentences \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     51\u001b[0m         (batch_indices\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), tokenized_suffix\u001b[39m.\u001b[39mrepeat(batch_indices\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)),\n\u001b[1;32m     52\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     53\u001b[0m     )\n\u001b[0;32m---> 55\u001b[0m     posterior\u001b[39m.\u001b[39mappend(get_logprob(v_sentences, model, stationary_dist))\n\u001b[1;32m     57\u001b[0m posterior \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(posterior)\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlog_softmax(posterior, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:26\u001b[0m, in \u001b[0;36mget_logprob\u001b[0;34m(input_ids, model, stationary_dist)\u001b[0m\n\u001b[1;32m     24\u001b[0m logprob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(stationary_dist[input_ids[:, \u001b[39m0\u001b[39m]])\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     logprob \u001b[39m=\u001b[39m logprob \u001b[39m+\u001b[39m get_cond_logprob(input_ids, model)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m logprob\n",
      "File \u001b[0;32m~/Dropbox/github/reverse-dynamics-nlp/reverse-llm-benchmarking/reverse_sampling.py:12\u001b[0m, in \u001b[0;36mget_cond_logprob\u001b[0;34m(input_ids, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cond_logprob\u001b[39m(input_ids, model):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Get conditional logprobs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m         logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[0;32m---> 12\u001b[0m             model(input_ids\u001b[39m=\u001b[39;49minput_ids)\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Get the log probabilities corresponding to the words in input_ids\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     relevant_logprobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(\n\u001b[1;32m     16\u001b[0m         logprobs, \u001b[39m2\u001b[39m, input_ids\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39m1\u001b[39m:]\n\u001b[1;32m     17\u001b[0m     )\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:763\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 763\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    764\u001b[0m     input_ids,\n\u001b[1;32m    765\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    766\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    767\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    768\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    769\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    770\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    771\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    772\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    773\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    774\u001b[0m )\n\u001b[1;32m    776\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    777\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:619\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    616\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_in(input_ids)\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dropout(inputs_embeds)\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/reversing-llms/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "suffix = \" The House Democrats oppose the impeachment of former President Barack Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\")\n",
    "\n",
    "loss = compute_loss_reverse_dynamics_reverse_prior(\n",
    "    model,\n",
    "    reverse_model,\n",
    "    tokenized_suffix,\n",
    "    vocab_batch_size=128,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverse_sampling import compute_loss_reverse_dynamics\n",
    "\n",
    "suffix = \" President Donald Trump filed a lawsuit against former President Barack Obama\"\n",
    "tokenized_suffix= tokenizer.encode(suffix, return_tensors=\"pt\").to(device)\n",
    "\n",
    "loss = compute_loss_reverse_dynamics(\n",
    "    model,\n",
    "    empirical_dist,\n",
    "    tokenized_suffix,\n",
    "    dilution=1.0,\n",
    "    vocab_batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Posterior vs Stationary Reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_dist = torch.ones_like(empirical_dist) / empirical_dist.shape[0]\n",
    "empirical_dist = empirical_dist * 0.7 + uniform_dist * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:13<00:00,  7.47it/s]\n",
      "100%|██████████| 99/99 [00:14<00:00,  7.02it/s]\n",
      "100%|██████████| 99/99 [00:17<00:00,  5.76it/s]\n",
      "100%|██████████| 99/99 [00:18<00:00,  5.39it/s]\n",
      "100%|██████████| 99/99 [00:24<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from reverse_sampling import sample_reverse_dynamics\n",
    "\n",
    "output1, logits1 = sample_reverse_dynamics(\n",
    "    model,\n",
    "    empirical_dist,\n",
    "    prefix_length,\n",
    "    tokenized_suffix,\n",
    "    temperature=0.7,\n",
    "    vocab_batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In thiserior pair, Obama'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:25<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:25<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:22<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:18<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:14<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "logits2 = sr.stationary_reverse_full_dist_suffix_calculation(model, empirical_dist, output1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-14.1750, -14.4474, -15.0450,  ..., -13.0240, -12.8964, -13.3337],\n",
       "        [-12.3171, -12.7534, -13.0377,  ..., -12.1253,  -9.1679, -14.2333],\n",
       "        [-13.4286, -12.3321, -11.0569,  ..., -10.6190, -11.5005, -12.7546],\n",
       "        [-11.3032, -11.4353, -13.2914,  ..., -12.1252, -11.5411, -13.5619],\n",
       "        [-13.5377, -15.0662,  -8.3655,  ..., -14.6411, -13.6813, -13.2714]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits1.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0518e-05, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(logits2 - logits1.log_softmax(dim=-1)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverse_sampling import *\n",
    "test_suffix = torch.ones((1, 40), dtype=torch.long).to(device)\n",
    "compute_loss_reverse_dynamics_reverse_prior(\n",
    "    model,\n",
    "    reverse_model,\n",
    "    test_suffix,\n",
    "    vocab_batch_size=5000,\n",
    "    dilution=0.0,  # 0.3\n",
    "    device=\"cuda\",\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverse_sampling import *\n",
    "compute_loss_reverse_dynamics_reverse_prior_target_memory(\n",
    "    model,\n",
    "    reverse_model,\n",
    "    test_suffix,\n",
    "    target_memory=1,\n",
    "    dilution=0.0,  # 0.3\n",
    "    device=\"cuda\",\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "  print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"Tensor\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse_llm_benchmarking/reverse_dynamics_test.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse_llm_benchmarking/reverse_dynamics_test.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m1000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse_llm_benchmarking/reverse_dynamics_test.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m1000\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexinf/Dropbox/github/reverse-dynamics-nlp/reverse_llm_benchmarking/reverse_dynamics_test.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test \u001b[39m=\u001b[39m [x,] \u001b[39m+\u001b[39;49m y\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"Tensor\") to list"
     ]
    }
   ],
   "source": [
    "# random torch array of shape [1000]\n",
    "import torch\n",
    "x = torch.ones(1000)\n",
    "y = torch.ones(1000)+1\n",
    "test = [x,] + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [torch.tensor(1), torch.tensor(2), torch.tensor(3)]\n",
    "test_list = [torch.tensor(4),]+test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4), tensor(1), tensor(2), tensor(3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return cost in seconds for 1B model\n",
    "def cost_estimator_1B(suffix_length):\n",
    "  return 10 + 140/29 * (suffix_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_estimator_410m(suffix_length):\n",
    "  return 5 + 58/29 * (suffix_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_estimator_1B(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.249999999999993\n",
      "13.291666666666666\n"
     ]
    }
   ],
   "source": [
    "# total runtime in hours\n",
    "num_examples = 50\n",
    "suffix_length = 30\n",
    "print((num_examples*sum([cost_estimator_1B(i) for i in range(1,suffix_length)]))/(60*60))\n",
    "print((num_examples*sum([cost_estimator_410m(i) for i in range(1,suffix_length)]))/(60*60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.666666666666668"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reversing-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
